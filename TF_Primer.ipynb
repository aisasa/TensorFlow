{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF Primer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wF5wszaj97Y"
      },
      "source": [
        "# TensorFlow/Keras: un poco más allá del 'Hola, mundo'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZSlp3DAjdYf",
        "cellView": "form"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szOi24YKC8xi"
      },
      "source": [
        "## Infraestructura del código"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnrWf3PCEzXL"
      },
      "source": [
        "Importamos la última versión de TensorFlow (2.x a marzo 2021) para acceder a todas sus herramientas. Podemos imprimir su versión como comprobación adicional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0trJmd6DjqBZ"
      },
      "source": [
        "# Imports:\n",
        "import tensorflow as tf\n",
        "#print(tf.__version__)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQF-6EFcDKXI"
      },
      "source": [
        "## Gestión de los datos de entrada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NAbSZiaoJ4z"
      },
      "source": [
        "A continuación cargamos el conjunto de datos [MNIST](http://yann.lecun.com/exdb/mnist/). Su formato corresponde a un tensor tridimensional de varios miles de imágenes de 28x28 píxeles cada una (es decir, cada imagen viene definida por una matriz de 28 filas, o vectores, de 28 píxeles cada una), como puede comprobarse activando en el código las sentencias `print` con el método `shape`. Puesto que necesitamos convertir cada imagen en un vector de 28x28 = 784 elementos para alimentar correctamente la capa de inputs, modificamos su formato mediante `reshape()`, a la vez que convertimos los enteros entre [0, 255] (que definen la escala de grises de cada imagen) en números en coma flotante en el intervalo [0.0, 1.0] dividiendo cada elemento por 255 (podemos comprobar el contenido de, por ejemplo, la primera imagen mediante `print(x_train[0])`). Este formato decimal será más adecuado a nuestros cálculos (comprobar la precisión final del modelo sin esta conversión). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FP5258xjs-v"
      },
      "source": [
        "# Manage data:\n",
        "# Load... \n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
        "#print(x_train.shape) # -> (60000, 28, 28)\n",
        "#print(x_test.shape) # -> (10000, 28, 28)\n",
        "\n",
        "# ...and format data\n",
        "x_train = x_train.reshape((60000, 28*28)) / 255.0 \n",
        "x_test = x_test.reshape((10000, 28*28)) / 255.0 \n",
        "#print(x_train[0])"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYWhAHQQDSc_"
      },
      "source": [
        "## El modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPZ68wASog_I"
      },
      "source": [
        "Establecemos ahora nuestro modelo, que corresponderá al tipo `Sequential`, el básico y omnipresente, de la API de Keras. Lo compondrán dos capas: una de entrada con 64 nodos (con una función de activación simple, `'relu'`, que elimina posibles términos negativos en el tensor de entrada convirtiéndolos en ceros; puede comprobarse su efecto eliminándola pues es opcional), y una de salida con 10 nodos que recogerán, cada uno de ellos, la probabilidad de que la imagen de entrada corresponda a cada uno de los 10 posibles dígitos con los que puede ser alimentado el modelo (activación `'softmax'`; es también opcional, pero es conveniente apreciar los efectos de su eliminación). Mientras que esta última capa, por definición de la solución del problema, debe ser obligatoriamente de este tamaño, la de entrada puede establecerse con un número de nodos mayor o menor. Es importante comprobar como varía la precisión del modelo modificando el tamaño de la capa de inputs (ver, por ejemplo, con 512 nodos; o con más; o con menos...).\n",
        "\n",
        "Completamos el modelo mediante su configuración con `compile()`, estableciendo un método de optimización (mecanismo por el que se actualizan los pesos o parámetros del modelo durante el entrenamiento), una función de pérdida (función que en el entrenamiento va comparando los resultados obtenidos por el modelo con los que debía haber obtenido, siendo el objetivo del modelo que esta pérdida o error sea mínima por medio de la función de optimización), y una métrica de la precisión que nuestro modelo va alcanzando y que registre su evolución convenientemente.\n",
        "\n",
        "Podremos experimentar con otros [optimizadores](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) según los que encontremos implementados en la API, y también podremos elegir una [función de pérdida](https://www.tensorflow.org/api_docs/python/tf/keras/losses) alternativa. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3IKyzTCDNGo"
      },
      "source": [
        "# Model set up:\n",
        "# Structure...\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(64, activation='relu'), \n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# ...and configuration\n",
        "model.compile(optimizer='rmsprop', # Try also 'adam', 'SGD'... (and compare) \n",
        "              loss= 'sparse_categorical_crossentropy', # Try also 'mean_absolute_error' or others (and compare)\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix4mEL65on-w"
      },
      "source": [
        "Como hemos visto, con media docena de sentencias (una decena de líneas efectivas de código), aunque no triviales, hemos podido establecer un completo modelo que nos permita resolver el problema de la identificación de dígitos MNIST. No todos los datos a los que nos enfrentemos serán tan sencillos de gestionar, ni todos los modelos resultarán tan básicos, pero los fundamentos de otros modelos más complejos y el flujo de trabajo podremos asentarlos probablemente en buena parte de lo que estamos viendo. Hay que mencionar que no todo el código está sujeto a buenas prácticas de programación: algunas sentencias son susceptibles de un cierto y conveniente desglose para una mejor estructura e interpretación de sus acciones, pero el objetivo es incidir en la potencia del conjunto TensorFlow/Keras y en su completa y ágil API de alto nivel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_ITnTmYDYkG"
      },
      "source": [
        "## Entrenamiento y evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIqHTSJvCwOa"
      },
      "source": [
        "Hemos llegado casi al final. Nos queda proceder al entrenamiento del modelo alimentándolo con las 60 000 imágenes _train_ del conjunto MNIST, cuyos resultados (outputs) irá comparando el modelo con sus correspondientes etiquetas y actuando en consecuencia sobre los parámetros variables del modelo. Repetiremos este proceso varias veces (`epochs=5`) para optimizar el aprendizaje, con un número de ciclos que podremos variar para comprobar si surte efectos su modificación en la precisión del modelo (spoiler: sí, sobre todo si lo reducimos).\n",
        "\n",
        "Podremos imprimir un resumen de la estructura del modelo, resumen que será más útil cuanto más complejo sea el modelo que construyamos. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7dTAzgHDUh7"
      },
      "source": [
        "# Model training\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeYj7S919HpH"
      },
      "source": [
        "Finalmente, evaluamos nuestro modelo con el conjunto de imágenes _test_ de MNIST y comprobaremos una precisión final en torno al 97-98% de identificaciones correctas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtYkBct289vx"
      },
      "source": [
        "# Model evaluation\n",
        "print('\\nEvaluation results [loss, accuracy]:', model.evaluate(x_test,  y_test, verbose=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4JfEh7kvx6m"
      },
      "source": [
        "Este tutorial está basado en los más básicos [tutoriales de TensorFlow](https://www.tensorflow.org/tutorials), así como en código de François Chollet en su libro Deep Learning con Python."
      ]
    }
  ]
}